{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import importlib\n",
    "import shutil\n",
    "import torch\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "\n",
    "def load_object(name, kwargs):\n",
    "    \"\"\"\n",
    "    Load objects dynamically given the object name and its arguments\n",
    "    :param name: str - object name, class name or function name\n",
    "    :param kwargs: dict - keyword arguments\n",
    "    :return: object\n",
    "    \"\"\"\n",
    "    object_module, object_name = name.rsplit(\".\", 1)\n",
    "    object_module = importlib.import_module(object_module)\n",
    "    fn = getattr(object_module, object_name)(**kwargs)\n",
    "    return fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_checkpoint(model_path):\n",
    "    \"\"\"\n",
    "    Load model given the model path\n",
    "    :param model_path: str - path to model\n",
    "    :return: tagger - arabiner.trainers.BaseTrainer - the tagger model\n",
    "             vocab - torchtext.vocab.Vocab - indexed tags\n",
    "             train_config - argparse.Namespace - training configurations\n",
    "    \"\"\"\n",
    "    with open(os.path.join(model_path, \"tag_vocab.pkl\"), \"rb\") as fh:\n",
    "        tag_vocab = pickle.load(fh)\n",
    "\n",
    "    # Load train configurations from checkpoint\n",
    "    train_config = Namespace()\n",
    "    with open(os.path.join(model_path, \"args.json\"), \"r\") as fh:\n",
    "        train_config.__dict__ = json.load(fh)\n",
    "\n",
    "    # Initialize the loss function, not used for inference, but evaluation\n",
    "    loss = load_object(train_config.loss[\"fn\"], train_config.loss[\"kwargs\"])\n",
    "\n",
    "    # Load BERT tagger\n",
    "    model = load_object(train_config.network_config[\"fn\"], train_config.network_config[\"kwargs\"])\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    # Update arguments for the tagger\n",
    "    # Attach the model, loss (used for evaluations cases)\n",
    "    train_config.trainer_config[\"kwargs\"][\"model\"] = model\n",
    "    train_config.trainer_config[\"kwargs\"][\"loss\"] = loss\n",
    "\n",
    "    tagger = load_object(train_config.trainer_config[\"fn\"], train_config.trainer_config[\"kwargs\"])\n",
    "    tagger.load(os.path.join(model_path, \"checkpoints\"))\n",
    "    return tagger, tag_vocab, train_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omarm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './content/output/tag_vocab.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load the checkpoint\u001b[39;00m\n\u001b[0;32m      9\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./content/output/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 10\u001b[0m tagger, tag_vocab, train_config \u001b[38;5;241m=\u001b[39m \u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Function to tokenize and tag a sentence\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_sentence\u001b[39m(sentence, tagger, tag_vocab, tokenizer):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Tokenize the sentence\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[1;34m(model_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_checkpoint\u001b[39m(model_path):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Load model given the model path\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    :param model_path: str - path to model\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m             train_config - argparse.Namespace - training configurations\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtag_vocab.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fh:\n\u001b[0;32m     10\u001b[0m         tag_vocab \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(fh)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Load train configurations from checkpoint\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './content/output/tag_vocab.pkl'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "# Load the checkpoint\n",
    "model_path = \"./content/output/\"\n",
    "tagger, tag_vocab, train_config = load_checkpoint(model_path)\n",
    "\n",
    "# Function to tokenize and tag a sentence\n",
    "def predict_sentence(sentence, tagger, tag_vocab, tokenizer):\n",
    "    # Tokenize the sentence\n",
    "    tokens = tokenizer(sentence, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # Convert token IDs to a list (assuming 'tagger.predict' expects a list of token IDs)\n",
    "    input_ids = tokens[\"input_ids\"][0].tolist()\n",
    "\n",
    "    # Make predictions using the tagger\n",
    "    predictions = tagger.predict(input_ids)\n",
    "\n",
    "    # Convert predicted tag IDs to tag strings\n",
    "    tags = [tag_vocab.itos[prediction] for prediction in predictions]\n",
    "\n",
    "    # Create a list of dictionaries containing word and corresponding predicted tag\n",
    "    entities = []\n",
    "    for token, tag in zip(tokens[\"input_ids\"][0], tags):\n",
    "        token_str = tokenizer.convert_ids_to_tokens(token.item())\n",
    "\n",
    "        if not token_str.startswith(\"##\"):\n",
    "            entities.append({\"word\": token_str, \"entity\": tag})\n",
    "        else:\n",
    "            entities[-1][\"word\"] += token_str[2:]\n",
    "\n",
    "    return entities\n",
    "\n",
    "# Example usage\n",
    "text = 'محمد محمد محمد أبو تريكة (مواليد 7 نوفمبر 1978) لاعب كرة قدم دولي مصري سابق'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"your-pretrained-model\")\n",
    "predictions = predict_sentence(text, tagger, tag_vocab, tokenizer)\n",
    "\n",
    "for item in predictions:\n",
    "    print(item[\"word\"] + \"\\t\" + item[\"entity\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
